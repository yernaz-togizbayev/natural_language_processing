# -*- coding: utf-8 -*-
"""Exercise_Sheet_2.ipynb

Automatically generated by Colab.

# Natural Language Processing

## Exercise Sheet 2
"""

#imports for all exercises
import nltk
nltk.download('inaugural')
nltk.download('names')
nltk.download('wordnet')
nltk.download('brown')
nltk.download('universal_tagset')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('udhr')

from nltk import FreqDist
from nltk import ConditionalFreqDist
from nltk import bigrams
from nltk.corpus import inaugural
from nltk.corpus import names
from nltk.corpus import wordnet as wn
from nltk.corpus import brown
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.corpus import udhr
import matplotlib.pyplot as plt

"""### Exercise 1

Plot the conditional frequency distribution of how the usage of the words "men", "women", and "people" has changed over time in the Inaugural Address Corpus.

"""

cfd = ConditionalFreqDist(
    (target, fileid[:4])
    for fileid in inaugural.fileids()
    for word in inaugural.words(fileid)
    for target in ['men', 'women', 'people']
    if word.lower() == target
)
plt.figure(figsize=(14, 8))
cfd.plot()
plt.show()

"""### Exercise 2

Plot a conditional frequency distribution over the Names Corpus that allows you to see which initial letters are more frequent for males vs. females.
"""

# Create a ConditionalFreqDist of initial letters based on gender
cfd = ConditionalFreqDist(
    (fileid, name[0])
    for fileid in names.fileids()
    for name in names.words(fileid)
)

# Plot the conditional frequency distribution
plt.figure(figsize=(14, 8))
cfd.plot()
plt.show()

"""### Exercise 3

What percentage of noun synsets have no hyponyms? You can get all noun synsets using `wn.all_synsets('n')`.
"""

# Get all noun synsets
nouns_in_wordnet = list(wn.all_synsets('n'))

# Count the number of noun synsets with no hyponyms using filter and lambda
no_hyponyms_counter = len(list(filter(lambda synset: not synset.hyponyms(), nouns_in_wordnet)))

# Calculate the percentage
total_noun_synsets = len(nouns_in_wordnet)
synset_percentage_without_hyponyms = (no_hyponyms_counter / total_noun_synsets) * 100 if total_noun_synsets > 0 else 0

print(f"Percentage of noun synsets with no hyponyms: {synset_percentage_without_hyponyms:.2f}%")

"""### Exercise 4

Define a function `supergloss(s)` that takes a synset $s$ as its argument and returns a string consisting of the concatenation of the definition of $s$, and the definitions of all the hypernyms and hyponyms of $s$. Apply the function to the synset "car.n.01".
"""

def supergloss(s):
  definitions = []

  # Add synset definitions
  if s.definition():
      definitions.append(s.definition())

  # Add hypernyms definitions
  for hypernym in s.hypernyms():
      if hypernym.definition():
          definitions.append(hypernym.definition())

  # Add hyponyms definitions
  for hyponym in s.hyponyms():
      if hyponym.definition():
          definitions.append(hyponym.definition())

  return "\n".join(definitions)

# Apply the function to the synset "car.n.01"
print(f"Supergloss for 'car.n.01':\n{supergloss(wn.synset('car.n.01'))}")

"""### Exercise 5

Define a function to find all words that occur at least $n$ times in the Brown Corpus. Call the function with the value $n=200$.
"""

def find_frequent_words(n: int) -> list[str]:
    fdist = FreqDist(word.lower() for word in brown.words())
    frequent_words = [word for word, freq in fdist.items() if freq >= n]
    return frequent_words

# Call the function with  value n=200
frequent_words_200 = find_frequent_words(200)

print(f"Words occurring at least 200 times in the Brown Corpus:\n{frequent_words_200}")

"""### Exercise 6

Write a program that lists the lexical diversity scores for all Brown Corpus genres, one per line.

"""

def lexical_diversity(text: list[str]) -> float:
  return len(set(text)) / len(text) if len(text) > 0 else 0

# Calculate and print the lexical diversity for each genre
print("Lexical diversity scores for Brown Corpus genres:")
for genre in brown.categories():
  genre_words = brown.words(categories=genre)
  diversity_score = lexical_diversity(genre_words)
  print(f"{genre}: {diversity_score:.4f}")

"""### Exercise 7

Write a function that finds the 50 most frequently occurring words of a text that are not stopwords. Apply the function to the "news" genre of the Brown Corpus.


"""

def find_most_frequent_non_stopwords(text: list[str]):
  english_stopwords = set(stopwords.words('english'))
  words = [word.lower() for word in text if word.isalpha() and word.lower() not in english_stopwords]
  fdist = FreqDist(words)
  return fdist.most_common(50)

# Apply the function to the "news" genre of the Brown Corpus
news_words = brown.words(categories='news')
frequent_non_stopwords_news = find_most_frequent_non_stopwords(news_words)

print(f"Top 50 most frequent non-stopwords in the 'news' genre:\n")
for word, frequency in frequent_non_stopwords_news:
  print(f"{word}: {frequency}")

"""### Exercise 8

Write a function that finds the 50 most frequently occurring bigrams of a text, omitting bigrams that contain stopwords. Apply the function to the "news" genre of the Brown Corpus.

"""

def find_frequent_non_stopwords_bigrams(text: list[str]):
    english_stopwords = set(stopwords.words('english'))
    words = [word.lower() for word in text if word.isalpha()]

    # Generate bigrams and filter out those containing stopwords
    filtered_bigrams = [
        (w1, w2) for w1, w2 in bigrams(words)
        if w1 not in english_stopwords and w2 not in english_stopwords
    ]

    # Calculate frequency distribution of the filtered bigrams
    fdist = FreqDist(filtered_bigrams)
    return fdist.most_common(50)

# Apply the function to the "news" genre of the Brown Corpus
news_words = brown.words(categories='news')
frequent_non_stopwords_bigrams_news = find_frequent_non_stopwords_bigrams(news_words)

print(f"Top 50 most frequent non-stopwords bigrams in the 'news' genre:\n")
for bigram, frequency in frequent_non_stopwords_bigrams_news:
  print(f"{bigram}: {frequency}")

"""### Exercise 9

Write a function `word_freq()` that takes a word and the name of a Brown Corpus genre as arguments, and computes the frequency of the word in that section of the corpus. Use the function to compute the frequency of "love" in "news" vs. "romance" genre.
"""

def word_freq(word: str, genre: str) -> float:
    genre_words = [word.lower() for word in brown.words(categories=genre)]
    fdist = FreqDist(genre_words)
    return fdist[word.lower()] / len(genre_words) if len(genre_words) > 0 else 0

# Compute the frequency of "love" in "news" vs. "romance" genre
love_freq_news = word_freq("love", "news")
love_freq_romance = word_freq("love", "romance")

print(f"Frequency of 'love' in the 'news' genre: {love_freq_news:.6f}")
print(f"Frequency of 'love' in the 'romance' genre: {love_freq_romance:.6f}")

"""### Exercise 10

Define a function `find_language()` that takes a string as its argument, and returns a list of languages that have that string as a word. Use the Universal Declaration of Human Rights Corpus and limit your searches to files in the Latin-1 encoding. Use the function to find the languages for the string "basis".

"""

def find_language(target_word: str) -> list[str]:
    languages = []
    for fileid in udhr.fileids():
        # Check if the file is in Latin-1 encoding
        if 'Latin1' in fileid:
            if target_word.lower() in (word.lower() for word in udhr.words(fileid)):
                # Extract the language name from the fileid
                language = fileid.split('-')[0]
                languages.append(language)
    return languages

# Find the languages for the string "basis"
languages_for_string_basis = find_language("basis")

print(f"Languages for the string 'basis': {languages_for_string_basis}")

"""### Exercise 11

What is the branching factor of the noun hypernym hierarchy, i.e. for every noun synset that has hyponyms, how many do they have on average?
"""

# Get all noun synsets
noun_synsets = list(wn.all_synsets('n'))

# Filter for synsets that have hyponyms and count their hyponyms
hyponym_counts = [len(ss.hyponyms()) for ss in noun_synsets if ss.hyponyms()]

# Calculate the average branching factor
average_branching_factor = sum(hyponym_counts) / len(hyponym_counts) if hyponym_counts else 0

print(f"Average branching factor of the noun hypernym hierarchy: {average_branching_factor:.2f}")